简称: RoBERTa
任务: 自然语言推理
论文:
  题目: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  作者:
  - Yinhan Liu
  - Myle Ott
  - Naman Goyal
  - Jingfei Du
  - Mandar Joshi
  - Danqi Chen
  - Omer Levy
  - Mike Lewis
  - Luke Zettlemoyer
  - Veselin Stoyanov
  单位:
  - Paul G. Allen School of Computer Science & Engineering University of Washington
  - Facebook AI
  发表年份: 2019
  卷（号）: arXiv:1907.11692
  出版社: arXiv preprint
  页码: None
  下载地址: https://arxiv.org/abs/1907.11692
  引用次数: 686
代码:
  地址: https://github.com/pytorch/fairseq
  时间: 2020
  star数量: 11k
  fork数量: 2.8k
  编程语言: Python>=3.6
  框架:
  - PyTorch>= 1.5.
  - NVIDIA GPU
  - NCCL
模型性能:
  数据集:
    GLUE:
      CoLA: 67.8
      SST-2: 96.7
      MRPC: 92.3
      STS-B: 92.2
      QQP: 90.2
      MNLI-m/mm: 90.8/90.2
      QNLI: 98.9
      RTE: 88.2
      WNLI: 89.0
      Avg: 88.5
    SQuAD v2.0:
      EM: 87.0
      F1: 89.9
    SQuAD v1.1:
      EM: 88.9
      F1: 94.6
    RACE:
      Accuracy: 83.2
      Middle: 86.5
      High: 81.3
关键技术: Masked language model pretraining
简介: 本文模型是BERT的改进版，使用了更大的模型参数量，更大bacth size，更多的训练数据。在训练方法上，去掉下一句预测(NSP)任务，使用了动态掩码和文本编码。
简介链接: https://blog.csdn.net/ljp1919/article/details/100666563
