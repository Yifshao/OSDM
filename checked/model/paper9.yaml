简称: UNILM
任务: 文本摘要
论文:
  题目: Unified Language Model Pre-training for Natural Language Understanding and Generation
  作者:
  - Li Dong
  - Nan Yang
  - Wenhui Wang
  - Furu Wei
  - Xiaodong Liu
  - Yu Wang
  - Jianfeng Gao
  - Ming Zhou
  - ' Hsiao-Wuen Hon'
  单位:
  - Microsoft Research
  发表年份: 2019
  卷（号）: NeurIPS-2019
  出版社: Curran Associates, Inc.
  页码: 13063--13075
  下载地址: https://arxiv.org/pdf/1905.03197.pdf
  引用次数: 305
代码:
  地址: https://github.com/microsoft/unilm
  时间: 2020
  star数量: 1723
  fork数量: 371
  编程语言: Python
  框架: UniLM v1
模型性能:
  数据集:
    Gigaword:
      ROUGE-1: 38.45
      ROUGE-2: 19.45
      ROUGE-L: 35.75
    CNN / Daily Mail:
      ROUGE-1: 43.33
      ROUGE-2: 20.21
      ROUGE-L: 40.51
    SQuAD v2.0:
      EM: 84.7
      F1: 87.6
    SQuAD v1.1:
      BLEU-4: 23.75
      MTR: 25.61
      ROUGE-L: 52.04
    CoQA:
      Extractive F1: 84.9
      Generative F1: 82.5
    GLUE:
      CoLA: 61.1
      SST-2: 94.5
      MRPC: 90.0
      STS-B: 87.7
      QQP: 71.7
      MNLI-m/mm: 87.0/85.9
      QNLI: 92.7
      RTE: 70.9
      WNLI: 65.1
      AX: 38.4
      Score: 80.8
关键技术: UNIfied pre-trained Language Model
简介: 本文提出了一种新的统一预训练语言模型（UNILM），该模型可以对自然语言理解和生成任务进行微调,在GLUE benchmark、SQuAD 2.0和CoQA上结果优于BERT且在5个自然语言数据集上实现SOTA
简介链接: https://blog.csdn.net/anonIsAlreadyTaken/article/details/102822774
