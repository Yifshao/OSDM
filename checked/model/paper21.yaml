简称: Longformer
任务: 自然语言推理
论文:
  题目: 'Longformer: The Long-Document Transformer'
  作者:
  - Iz Beltagy
  - Matthew E. Peters
  - Arman Cohan
  单位:
  - Allen Institute for Artificial Intelligence
  发表年份: 2020
  下载地址: https://arxiv.org/pdf/2004.05150.pdf
  引用次数: 166
代码:
  地址: https://github.com/allenai/longformer
  时间: 2020
  star数量: 1k
  fork数量: 125
  编程语言: Python 3.7
  框架:
  - cudatoolkit=10.0
模型性能:
  数据集:
    WikiHop:
      Acc: 75.0
    TriviaQA:
      F1: 75.2
    HotpotQA:
      F1: 64.4
    OntoNotes:
      F1: 78.6
    IMDB:
      Acc: 85.7
    Hyperpartisan:
      F1: 94.8
关键技术: An attention mechanism that scales linearly with sequence length
简介: 本文提出的Longformer，改进了Transformer的传统attention机制：对于每一个token，只对固定窗口大小的附近token计算local
  attention，并结合具体任务，计算少量的global attention。本文复杂度较低，通用性强且部署容易。
简介链接: https://blog.csdn.net/xixiaoyaoww/article/details/107398795
