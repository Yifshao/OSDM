简称: ALBERT
任务: 自然语言推理
论文:
  题目: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  作者:
  - Zhenzhong Lan
  - Mingda Chen
  - Sebastian Goodman
  - Kevin Gimpel
  - Piyush Sharma
  - Radu Soricut
  单位:
  - Google Research
  - Toyota Technological Institute at Chicago
  发表年份: 2019
  卷（号）: ICLR-2020
  出版社: International Conference on Learning Representations
  页码: None
  下载地址: https://openreview.net/forum?id=H1eA7AEtvS
  引用次数: 1093
代码:
  地址: https://github.com/google-research/ALBERT
  时间: 2020
  star数量: 2.6k
  fork数量: 475
  编程语言: Python
  框架:
  - PyTorch
模型性能:
  数据集:
    GLUE:
      CoLA: 71.4
      SST-2: 96.9
      MRPC: 90.9
      STS-B: 93.0
      QQP: 92.2
      MNLI: 90.8
      QNLI: 95.3
      RTE: 89.2
    SQuAD v2.0:
      EM: 89.7
      F1: 92.2
    SQuAD v1.1:
      EM: 90.1
      F1: 95.5
    RACE:
      Accuracy: 89.4
      Middle: 91.2
      High: 88.6
关键技术: Factorized embedding parameterization and Cross-layer parameter sharing
简介: 预训练自然语言表征时，增加模型大小一般是可以提升模型在下游任务中的性能。但是进一步增加模型大小将遇到以下困难：(1)GPU/TPU内存不足(2)训练时间会更长(3)模型退化。为了解决上述这些问题，本文提出通过两种参数精简技术来降低内存消耗，并加快BERT的训练速度。此外，本文还引入一个自监督损失，用于对句子连贯性建模，并证明该损失函数能够提升多句子作为输入的下游任务的性能。
简介链接: https://blog.csdn.net/ljp1919/article/details/101680220
