简称: GPT
任务: 自然语言推理
论文:
  题目: Improving Language Understanding by Generative Pre-Training
  作者:
  - Alec Radford
  - Karthik Narasimhan
  - Tim Salimans
  - Ilya Sutskever
  单位:
  - OpenAI
  发表年份: 2018
  下载地址: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
  引用次数: 1844
代码:
  地址: https://github.com/openai/finetune-transformer-lm
  时间: 2018
  star数量: 1.6k
  fork数量: 419
  编程语言: Python
  框架:
  - ftfy==4.4.3
  - spacy
模型性能:
  数据集:
    GLUE:
      CoLA: 47.9
      SST-2: 92.0
      MRPC: 84.9
      STS-B: 83.2
      QQP: 70.3
      MNLI: 81.8
      QNLI: 88.1
      RTE: 56.0
      Avg: 75.0
关键技术: Generative pre-training and discriminative fine-tuning
简介: 为了方便nlp的迁移学习，人们提出采用无标注数据训练语言模型（language model），并在其后加上一层全连接和softmax组成分类器，用有标注数据fine-tuning分类器。本文不使用rnn而使用transformer
  decoder做语言模型，并且将模型的input根据task做一定的转变，以期待分类器基本不变。
简介链接: https://blog.csdn.net/manmanxiaowugun/article/details/83794454
