简称: None
任务: 文本摘要
论文:
  题目: Text Summarization with Pretrained Encoders
  作者:
  - Yang Liu
  - Mirella Lapata
  单位:
  - University of Edinburgh
  发表年份: 2019
  卷（号）: EMNLP-2019
  出版社: Association for Computational Linguistics
  页码: 3730--3740
  下载地址: https://www.aclweb.org/anthology/D19-1387.pdf
  引用次数: 233
代码:
  地址: https://github.com/nlpyang/PreSumm
  时间: 2020
  star数量: 825
  fork数量: 325
  编程语言: Python 3.6
  框架: PyTorch 1.1.0
模型性能:
  数据集:
    CNN / Daily Mail:
      ROUGE-1: 43.85
      ROUGE-2: 20.34
      ROUGE-L: 39.9
    NYT:
      ROUGE-1: 49.02
      ROUGE-2: 31.02
      ROUGE-L: 45.55
    X-Sum:
      ROUGE-1: 38.81
      ROUGE-2: 16.5
      ROUGE-L: 31.27
关键技术: Bidirectional Encoder Representations from Transformers
简介: 文章将BERT有效地应用于文本摘要,介绍了一种基于BERT的文档级编码器，它能够表达文档的语义并获得文档的关键句。
简介链接: https://blog.csdn.net/alzy133/article/details/106365411
