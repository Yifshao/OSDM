简称: MPNet
任务: 自然语言推理
论文:
  题目: 'MPNet: Masked and Permuted Pre-training for Language Understanding'
  作者:
  - Kaitao Song
  - Xu Tan
  - Tao Qin
  - Jianfeng Lu
  - Tie-Yan Liu
  单位:
  - Nanjing University of Science and Technology
  - Microsoft Research
  发表年份: 2020
  卷（号）: NEURIPS-2020
  出版社: Curran Associates, Inc.
  页码: 16857--16867
  下载地址: https://arxiv.org/abs/2004.09297
  引用次数: 8
代码:
  地址: https://github.com/microsoft/MPNet
  时间: 2020
  star数量: 162
  fork数量: 18
  编程语言: Python
  框架:
  - pytorch_transformers==1.0.0
  - transformers
  - scipy
  - sklearn
模型性能:
  数据集:
    GLUE:
      CoLA: 64.0
      SST-2: 96.0
      MRPC: 89.1
      STS-B: 90.7
      QQP: 89.9
      MNLI: 88.5
      QNLI: 93.1
      RTE: 81.0
      Avg: 86.5
    SQuAD v2.0:
      EM: 82.8
      F1: 85.8
    SQuAD v1.1:
      EM: 86.9
      F1: 92.7
    RACE:
      Accuracy: 76.1
      Middle: 79.7
      High: 74.5
关键技术: Masked and permuted language modeling
简介: 本文提出了一种新的预训练方法MPNet，它继承了BERT和XLNet的优点，避免了它们的局限性。MPNet通过置换语言建模（与BERT中的MLM相比）来利用预测标记之间的依赖关系，并将辅助位置信息作为输入，使模型看到完整的句子，从而减少位置差异（与XLNet中的PLM相比）。
简介链接: https://blog.csdn.net/qq_27590277/article/details/109349280
