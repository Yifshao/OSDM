简称: LUKE
任务: 'NER'
论文: 
  题目: 'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention'
  作者:
    - Ikuya Yamada
    - Akari Asai
    - Hiroyuki Shindo
    - Hideaki Takeda
    - Yuji Matsumoto
  单位:
    - Studio Ousia
    - RIKEN AIP
    - University of Washington
    - Nara Institute of Science and Technology
    - National Institute of Informatics
  发表年份: 2020
  卷（号）: None
  出版社: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
  页码: 6442--6454
  下载地址: https://www.aclweb.org/anthology/2020.emnlp-main.523.pdf
  引用次数: 5
代码:
  地址: https://github.com/studio-ousia/luke
  时间: 2020
  star数量: 149
  fork数量: 16
  编程语言: Python
  框架: Pytorch
模型性能: 
  数据集:
    - Open Entity:
      - Prec.: 79.9
      - Rec.: 76.6
      - F1: 78.2
    - TACRED:
      - Prec.: 70.4
      - Rec.: 75.1
      - F1: 72.7
    - CoNLL 2003:
      - F1: 94.3
    - ReCoRD:
      - Dev EM: 90.8
      - Dev F1: 91.4
      - Test EM: 90.6
      - Test F1: 91.2
    - SQuAD v1.1:
      - Dev EM: 89.8
      - Dev F1: 95.0
      - Test EM: 90.2
      - Test F1: 95.4
关键技术: 嵌入矩阵
简介: '作者提出了一种具备与训练任务的Transformer模型——LUKE（基于知识嵌入的语言理解）：MLM+预测文档中被掩盖的实体。作者保持一个实体嵌入矩阵（500K不同的实体），又添加了实体感知（entity-aware）的自注意机制。这个自注意机制在本质上是三个查询矩阵，取决于所计算的token类型（单词-实体，实体-实体，实体-单词）。一个简单的扩充就可以实现新的下游任务，并且相对于RoBERTa和最近的KG增强基线略微改进。'
简介链接: 'http://www.360doc.com/content/20/1211/19/7673502_950874869.shtml'

  




